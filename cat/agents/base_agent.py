import re
from typing import List, Dict, Tuple
from abc import ABC, abstractmethod
from langchain_core.messages import BaseMessage
from langchain_core.documents import Document
from pydantic import Field

from cat import utils
from cat.experimental.form import CatForm
from cat.looking_glass import prompts
from cat.mad_hatter.decorators import CatTool


class LLMAction(utils.BaseModelDict):
    """
    Represents an action (tool call) requested by the LLM.

    This class encapsulates the details of a tool or function call that
    an LLM wants to execute, including input parameters, output results,
    and execution metadata.

    Attributes
    ----------
    id : str | None, optional
        Unique identifier for the action call, used to match requests
        with responses in LLM API interactions, by default None.
    name : str
        The name of the tool or function to call.
    input : Dict
        Dictionary containing the input parameters for the tool call.
    output : str | None, optional
        The result of executing the tool call, by default None.
    return_direct : bool, optional
        Whether the tool output should be returned directly to the user
        without further LLM processing, by default False.

    Notes
    -----
    The id field is crucial for matching tool call requests with responses.
    This is a strict requirement for LLM APIs that support tool calling,
    """
    id: str | None = None
    name: str
    input: Dict | None = None
    output: str | None = None
    return_direct: bool = False


class AgentInput(utils.BaseModelDict):
    """
    Represents the input to an agent including context documents, user input, and chat history.

    Attributes
    ----------
    context : List[Document]
        List of context documents relevant to the agent's task.
    input : str
        The user's input message to the agent.
    history : List[BaseMessage], optional
        The chat history as a list of messages, by default [].
    """
    context: List[Document]
    input: str
    history: List[BaseMessage] = Field(default_factory=list)


class AgentOutput(utils.BaseModelDict):
    """
    Represents the output from an agent execution including text and actions.

    Attributes
    ----------
    output : str | None, optional
        The textual output generated by the agent, by default None.
    actions : List[LLMAction], optional
        List of actions (tool calls) executed by the agent, by default [].
    with_llm_error : bool, optional
        Indicates if there was an error during LLM processing, by default False.
    """
    output: str | None = None
    actions: List[LLMAction] = []
    with_llm_error: bool = False

    @property
    def intermediate_steps(self) -> List[Tuple[Tuple[str, Dict], str]]:
        """Return the list of actions as intermediate steps for compatibility."""
        return [((action.name, action.input), action.output) for action in self.actions]


class CatAgent(ABC):
    def __init__(self, stray):
        # important so all agents have the session and utilities at disposal
        # if you subclass and override the constructor, remember to set it or call super()
        self._stray = stray
        self._plugin_manager = stray.cheshire_cat.plugin_manager

    def _get_system_prompt(self) -> str:
        # obtain prompt parts from plugins
        prompt_prefix = self._plugin_manager.execute_hook(
            "agent_prompt_prefix", prompts.MAIN_PROMPT, cat=self._stray
        )
        prompt_suffix = self._plugin_manager.execute_hook(
            "agent_prompt_suffix", "", cat=self._stray
        )

        return prompt_prefix + prompt_suffix

    def _get_procedures(self) -> List[CatTool | CatForm]:
        """Get both plugins' tools and MCP tools in CatTool format, plus internal forms."""
        mcp_tools = [] #await self.cat.mcp.list_tools()
        internal_procedures = self._plugin_manager.procedures

        tools = mcp_tools + internal_procedures

        return tools

    async def execute(self, *args, **kwargs) -> AgentOutput:
        """
        Execute the agents.

        Args:
            stray: StrayCat
                Stray Cat instance containing the working memory and the chat history.

        Returns:
            agent_output: AgentOutput
                Reply of the agent, instance of AgentOutput.
        """
        def clean_response(response: str) -> str:
            # parse the `response` string and get the text from <answer></answer> tag
            if "<answer>" in response and "</answer>" in response:
                start = response.index("<answer>") + len("<answer>")
                end = response.index("</answer>")
                return response[start:end].strip()
            # otherwise, remove from `response` all the text between whichever tag and then return the remaining string
            # This pattern matches any complete tag pair: <tagname>content</tagname>
            cleaned = re.sub(r'<([^>]+)>.*?</\1>', '', response, flags=re.DOTALL)
            return cleaned.strip()

        # prepare input to be passed to the agent.
        #   Info will be extracted from working memory
        # Note: agent_input works both as a dict and as an object
        latest_n_history = kwargs.get("latest_n_history", 5) * 2  # each interaction has user + cat message

        agent_input = AgentInput(
            context=[m.document for m in self._stray.working_memory.declarative_memories],
            input=self._stray.working_memory.user_message.text,
            history=[h.langchainfy() for h in self._stray.working_memory.history[-latest_n_history:]]
        )
        agent_input = utils.restore_original_model(
            self._plugin_manager.execute_hook("before_agent_starts", agent_input, cat=self._stray), AgentInput
        )

        # should we run the default agents?
        agent_fast_reply = utils.restore_original_model(
            self._plugin_manager.execute_hook("agent_fast_reply", {}, cat=self._stray),
            AgentOutput
        )
        if agent_fast_reply and agent_fast_reply.output:
            return agent_fast_reply

        llm_output = await self.execute_llm(agent_input)
        if type(llm_output) is str:
            # simple string message
            return AgentOutput(output=clean_response(llm_output))

        if type(llm_output) is LLMAction:
            procedures = self._get_procedures()

            # LLM has chosen a tool or a form, run it to get the output
            for procedure in procedures:
                if procedure.name == llm_output.name:
                    # update the action with an output, actually executing the tool / form
                    llm_output = utils.dispatch_event(procedure.execute, stray=self._stray, action=llm_output)

            return AgentOutput(output=llm_output.output, actions=[llm_output])

        return AgentOutput()

    @abstractmethod
    async def execute_llm(self, agent_input: AgentInput) -> str | LLMAction:
        """
        Abstract method to execute the LLM with the given agent input.

        This method should be implemented by subclasses to define how the
        LLM is called and how it processes the agent input.

        Args:
            agent_input: AgentInput
                The input to the agent including context documents, user input, and chat history.

        Returns:
            str | LLMAction
                The output from the LLM, which can be a simple string message or an LLMAction.
        """
        pass

    def __str__(self):
        return self.__class__.__name__
