import re
from typing import List, Dict, Tuple
from langchain_core.messages import BaseMessage
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate
from pydantic import Field

from cat import utils
from cat.templates import prompts
from cat.mad_hatter import CatProcedure


class LLMAction(utils.BaseModelDict):
    """
    Represents an action taken by the LLM, such as calling a tool or function.

    Attributes
    ----------
    output : str
        The result of executing the LLM action.
    tools : List[Dict] | None, optional
        List of available tools or functions that can be called, by default [].

    Notes
    -----
    The id field is crucial for matching tool call requests with responses.
    This is a strict requirement for LLM APIs that support tool calling,
    """
    output: str | None = None
    tools: List[Dict] | None = []


class AgentInput(utils.BaseModelDict):
    """
    Represents the input to an agent including context documents, user input, and chat history.

    Attributes
    ----------
    context : List[Document]
        List of context documents relevant to the agent's task.
    input : str
        The user's input message to the agent.
    history : List[BaseMessage], optional
        The chat history as a list of messages, by default [].
    """
    context: List[Document]
    input: str
    history: List[BaseMessage] = Field(default_factory=list)


class AgentOutput(utils.BaseModelDict):
    """
    Represents the output from an agent execution including text and actions.

    Attributes
    ----------
    output : str | None, optional
        The textual output generated by the agent, by default None.
    action : LLMAction, optional
        List of an LLM action with eventual tool calls executed by the agent, by default None.
    with_llm_error : bool, optional
        Indicates if there was an error during LLM processing, by default False.
    """
    output: str | None = None
    action: LLMAction | None = None
    with_llm_error: bool = False

    @property
    def intermediate_steps(self) -> List[Tuple[Tuple[str, Dict], str]]:
        """Return the list of actions as intermediate steps for compatibility."""
        return [
            ((tool["name"], tool["args"]), tool["output"]) for tool in self.action.tools
        ] if self.action else []


class Dormouse:
    """
    The Dormouse agent. It uses the LLM to process user input, access tools and forms,
    and generate responses based on context and history. It integrates with plugins
    to extend its capabilities. It is designed to be flexible and extensible.

    Attributes
    ----------
    stray : Stray
        The main Stray instance providing access to session and utilities.
    _plugin_manager : PluginManager
        The plugin manager for handling plugins and their hooks.

    Methods
    -------
    execute(*args, **kwargs) -> AgentOutput
        Executes the agent with the given arguments and returns the output.
    """
    def __init__(self, stray):
        # important so all agents have the session and utilities at disposal
        # if you subclass and override the constructor, remember to set it or call super()
        self._stray = stray
        self._plugin_manager = stray.cheshire_cat.plugin_manager

    def _get_system_prompt(self) -> str:
        # obtain prompt parts from plugins
        prompt_prefix = self._plugin_manager.execute_hook(
            "agent_prompt_prefix", prompts.MAIN_PROMPT, cat=self._stray
        )
        prompt_suffix = self._plugin_manager.execute_hook(
            "agent_prompt_suffix", "", cat=self._stray
        )

        return prompt_prefix + prompt_suffix

    def _get_procedures(self) -> List[CatProcedure]:
        """Get both plugins' tools and MCP tools in CatTool format, plus internal forms."""
        mcp_tools = []  # await self.cat.mcp.list_tools()

        # Tools are already instances, keep them as is
        tools = mcp_tools + self._plugin_manager.tools

        # Forms need to be instantiated
        form_instances = []
        for form_class in self._plugin_manager.forms:
            # Create form instance with stray reference
            form_instance = form_class(self._stray)
            form_instances.append(form_instance)

        procedures = tools + form_instances
        return procedures

    def _clean_response(self, response: str) -> str:
        # parse the `response` string and get the text from <answer></answer> tag
        if "<answer>" in response and "</answer>" in response:
            start = response.index("<answer>") + len("<answer>")
            end = response.index("</answer>")
            return response[start:end].strip()

        # otherwise, remove from `response` all the text between whichever tag and then return the remaining string
        # This pattern matches any complete tag pair: <tagname>content</tagname>
        cleaned = re.sub(r'<([^>]+)>.*?</\1>', '', response, flags=re.DOTALL)

        return cleaned.strip()

    async def execute(self, *args, **kwargs) -> AgentOutput:
        """
        Execute the agents.

        Returns:
            agent_output: AgentOutput
                Reply of the agent, instance of AgentOutput.
        """
        # prepare input to be passed to the agent.
        #   Info will be extracted from working memory
        # Note: agent_input works both as a dict and as an object
        latest_n_history = kwargs.get("latest_n_history", 5) * 2  # each interaction has user + cat message

        agent_input = utils.restore_original_model(
            self._plugin_manager.execute_hook(
                "before_agent_starts",
                AgentInput(
                    context=[m.document for m in self._stray.working_memory.declarative_memories],
                    input=self._stray.working_memory.user_message.text,
                    history=[h.langchainfy() for h in self._stray.working_memory.history[-latest_n_history:]]
                ),
                cat=self._stray
            ),
            AgentInput
        )

        # should we run the default agents?
        agent_fast_reply = utils.restore_original_model(
            self._plugin_manager.execute_hook("agent_fast_reply", {}, cat=self._stray),
            AgentOutput
        )
        if agent_fast_reply and agent_fast_reply.output:
            return agent_fast_reply

        # obtain prompt parts from plugins
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessagePromptTemplate.from_template(template=self._get_system_prompt()),
                *agent_input.history,
            ]
        )

        # Format the prompt template with the actual values to get a string
        # Convert to string - this will combine all messages into a single string
        llm_output = await self._stray.llm(
            prompt,
            prompt_variables={"context": agent_input.context, "input": agent_input.input},
            procedures=self._get_procedures(),
            stream=True,
            caller_return_short=True,
            caller_skip=2,
        )

        agent_output = self._clean_response(llm_output.output)
        if len(llm_output.tools) == 0:
            # No tool calls, update chat response and exit
            return AgentOutput(output=agent_output)

        for tool_call in llm_output.tools:
            # LLM has chosen a tool or a form, run it to get the output
            for procedure in self._get_procedures():
                if procedure.name != tool_call["name"]:
                    continue

                # update the action with an output, actually executing the tool / form
                tool_call["output"] = await procedure.execute(stray=self._stray, tool_call=tool_call)
                if procedure.return_direct:
                    # tool wants a return_direct, update chat response and get out
                    return AgentOutput(output=tool_call["output"], action=llm_output)

                # append tool request and tool output messages
                agent_output += f"\n{tool_call['output']}"

        return AgentOutput(output=agent_output.strip(), action=llm_output)

    def __str__(self):
        return self.__class__.__name__
