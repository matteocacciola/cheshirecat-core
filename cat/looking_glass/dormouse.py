import re
from typing import List, Dict, Tuple, Any
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.prompts import BasePromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableConfig

from cat import utils
from cat.log import log
from cat.mad_hatter import CatProcedure


class LLMAction(utils.BaseModelDict):
    """
    Represents an action taken by the LLM, such as calling a tool or function.

    Attributes
    ----------
    output : str
        The result of executing the LLM action.
    tools : List[Dict] | None, optional
        List of available tools or functions that can be called, by default [].

    Notes
    -----
    The id field is crucial for matching tool call requests with responses.
    This is a strict requirement for LLM APIs that support tool calling,
    """
    output: str | None = None
    tools: List[Dict] | None = []


class AgentOutput(utils.BaseModelDict):
    """
    Represents the output from an agent execution including text and actions.

    Attributes
    ----------
    output : str | None, optional
        The textual output generated by the agent, by default None.
    action : LLMAction, optional
        List of an LLM action with eventual tool calls executed by the agent, by default None.
    with_llm_error : bool, optional
        Indicates if there was an error during LLM processing, by default False.
    """
    output: str | None = None
    action: LLMAction | None = None
    with_llm_error: bool = False

    @property
    def intermediate_steps(self) -> List[Tuple[Tuple[str, Dict], str]]:
        """Return the list of actions as intermediate steps for compatibility."""
        return [
            ((tool["name"], tool["args"]), tool["output"]) for tool in self.action.tools
        ] if self.action else []


class Dormouse:
    """
    The Dormouse agent. It uses the LLM to process user input, access tools and forms,
    and generate responses based on context and history. It integrates with plugins
    to extend its capabilities. It is designed to be flexible and extensible.

    Attributes
    ----------
    stray : Stray
        The main Stray instance providing access to session and utilities.
    _plugin_manager : PluginManager
        The plugin manager for handling plugins and their hooks.

    Methods
    -------
    execute(*args, **kwargs) -> AgentOutput
        Executes the agent with the given arguments and returns the output.
    """
    def __init__(self, stray):
        # important so all agents have the session and utilities at disposal
        # if you subclass and override the constructor, remember to set it or call super()
        self._stray = stray
        self._plugin_manager = stray.cheshire_cat.plugin_manager

    def _clean_response(self, response: str) -> str:
        # parse the `response` string and get the text from <answer></answer> tag
        if "<answer>" in response and "</answer>" in response:
            start = response.index("<answer>") + len("<answer>")
            end = response.index("</answer>")
            return response[start:end].strip()

        # otherwise, remove from `response` all the text between whichever tag and then return the remaining string
        # This pattern matches any complete tag pair: <tagname>content</tagname>
        cleaned = re.sub(r'<([^>]+)>.*?</\1>', '', response, flags=re.DOTALL)

        return cleaned.strip()

    async def _langchain_run(
        self,
        prompt: BasePromptTemplate,
        prompt_variables: Dict[str, Any] = None,
        procedures: List[CatProcedure] = None,
        callbacks: List[BaseCallbackHandler] = None,
    ) -> LLMAction:
        """
        Internal method to run the LLM with LangChain, handling tool binding if supported.

        Args:
            prompt : BasePromptTemplate
                The prompt template to use for the LLM.
            prompt_variables : Dict[str, Any], optional
                Variables to fill in the prompt template, by default None.
            procedures : List[CatProcedure], optional
                List of procedures (tools/forms) to bind to the LLM, by default None.
            callbacks : List[BaseCallbackHandler], optional
                List of callback handlers for logging and monitoring, by default None.

        Returns:
            LLMAction
                The action taken by the LLM, including output and any tool calls.
        """
        llm_to_use = self._stray.large_language_model

        # Add callbacks from plugins
        self._plugin_manager.execute_hook("llm_callbacks", callbacks, cat=self)

        # Intrinsic detection of tool binding support
        should_bind_tools = False
        if procedures and hasattr(llm_to_use, "bind_tools"):
            # For non-Ollama models, assume they support tool binding if they have the method
            should_bind_tools = True

            # Check if this is an Ollama model that might not support tools
            if hasattr(llm_to_use, "_client"):
                supports_tools = (
                        hasattr(llm_to_use, "format_tool_to_ollama") or
                        hasattr(llm_to_use, "_convert_tools_to_ollama_tools") or
                        getattr(llm_to_use, "supports_tool_calling", False)
                )

                # Additionally, check if the model has tool-related methods that indicate support
                tool_methods = [
                    method
                    for method in dir(llm_to_use)
                    if "tool" in method.lower() and "support" in method.lower()
                ]
                should_bind_tools = supports_tools or len(tool_methods) > 0

        # Attempt tool binding only if intrinsically supported
        if should_bind_tools:
            try:
                llm_to_use = llm_to_use.bind_tools(
                    [p.langchainfy() for p in procedures]
                )
            except Exception as e:
                model_id = getattr(llm_to_use, "model", "unknown")
                log.warning(f"Tool binding failed for model {model_id} → running without procedures. Error: {e}")
        elif procedures:
            log.debug("Model does not intrinsically support procedures binding → skipping tools.")

        chain = (
                prompt
                | RunnableLambda(lambda x: log.langchain_log_prompt(x, f"{self.__class__.__name__} prompt"))
                | llm_to_use
                | RunnableLambda(lambda x: log.langchain_log_output(x, f"{self.__class__.__name__} prompt output"))
        )

        # in case we need to pass info to the template
        langchain_msg = await chain.ainvoke(prompt_variables or {}, config=RunnableConfig(callbacks=callbacks))
        langchain_msg_content = getattr(langchain_msg, "content", str(langchain_msg))

        # if no tools involved, just return the string
        llm_output = LLMAction(output=langchain_msg_content)
        if hasattr(langchain_msg, "tool_calls") and len(langchain_msg.tool_calls) > 0:
            llm_output.tools = langchain_msg.tool_calls

        return llm_output

    async def run(
        self,
        prompt: BasePromptTemplate,
        prompt_variables: Dict[str, Any] = None,
        procedures: List[CatProcedure] = None,
        callbacks: List[BaseCallbackHandler] = None,
    ) -> AgentOutput:
        procedures = procedures or []

        llm_output = await self._langchain_run(
            prompt=prompt,
            prompt_variables=prompt_variables or {},
            procedures=procedures,
            callbacks=callbacks or [],
        )

        agent_output = self._clean_response(llm_output.output.strip())
        if len(llm_output.tools) == 0:
            # No tool calls, update chat response and exit
            return AgentOutput(output=agent_output)

        for tool_call in llm_output.tools:
            # LLM has chosen a tool or a form, run it to get the output
            for procedure in procedures:
                if procedure.name != tool_call["name"]:
                    continue

                # update the action with an output, actually executing the tool / form
                tool_call["output"] = await procedure.execute(stray=self._stray, tool_call=tool_call)
                if procedure.return_direct:
                    # tool wants a return_direct, update chat response and get out
                    return AgentOutput(output=tool_call["output"], action=llm_output)

                # append tool request and tool output messages
                agent_output += f"\n{tool_call['output']}"

        return AgentOutput(output=self._clean_response(agent_output.strip()), action=llm_output)

    def __str__(self):
        return self.__class__.__name__
