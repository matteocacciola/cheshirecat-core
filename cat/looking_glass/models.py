from io import BytesIO
from typing import List, Tuple, Dict, Any
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage
from langchain_core.tools import StructuredTool
from pydantic import BaseModel, Field, ConfigDict

from cat.services.memory.messages import CatMessage


class AgenticWorkflowTask(BaseModel):
    """
    Represents the input to an agentic workflow including context documents, user input, and chat history.

    Attributes
    ----------
    user_prompt: str
        The user's input message to the agent.
    system_prompt: str
        The system prompt to be used by the agent.
    prompt_variables: Dict, optional
        Additional variables for prompt formatting, by default {}.
    context: List[Document]
        List of context documents relevant to the agent's task.
    history: List[BaseMessage], optional
        The chat history as a list of messages, by default [].
    tools: List[StructuredTool], optional
        A list of tools available to the agent, by default [].
    """
    user_prompt: str
    system_prompt: str | None = None
    prompt_variables: Dict | None = Field(default_factory=dict)
    context: List[Document] | None = Field(default_factory=list)
    history: List[BaseMessage] | None = Field(default_factory=list)
    tools: List[StructuredTool] | None = Field(default_factory=list)

    def __init__(self, **data: Any):
        super().__init__(**data)
        self.prompt_variables.update({"context": self.context})


class AgenticWorkflowOutput(BaseModel):
    """
    Represents the output from an execution of an agentic workflow, including text and actions.

    Attributes
    ----------
    output: str | None, optional
        The textual output generated by the agent, default None.
    intermediate_steps: List, optional
        Intermediate steps taken during the agent's execution, default None.
    with_llm_error: bool, optional
        Indicates if there was an error during LLM processing, default False.
    """
    output: str | None = None
    intermediate_steps: List[Tuple[Tuple[str | None, Dict, Dict] | None, str]] = Field(default_factory=list)
    with_llm_error: bool = False


class StoredSourceWithMetadata(BaseModel):
    """
    Represents a stored source of Knowledge Base along with its metadata.

    Attributes
    ----------
    name: str
        The unique identifier of the stored source, e.g., a file path or URL.
    content: BytesIO | None
        The content of the source as a BytesIO stream, or None if the element is an URL
    metadata: Dict
        A dictionary containing metadata associated with the source.
    """
    name: str
    content: BytesIO | None
    metadata: Dict

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def __eq__(self, other: "StoredSourceWithMetadata") -> bool:
        return self.name == other.name

    def __hash__(self) -> int:
        return hash(self.name)


class ChatResponse(BaseModel):
    agent_id: str
    user_id: str
    chat_id: str
    message: CatMessage


# Empty class to represent the basic plugin Settings model
class PluginSettingsModel(BaseModel):
    pass


class PluginManifest(BaseModel):
    id: str
    name: str = "Unknown"
    version: str = "0.0.0"
    thumb: str = None
    tags: str = "Unknown"
    description: str = (
        "Description not found for this plugin. Please create a plugin.json manifest in the plugin folder."
    )
    author_name: str = "Unknown"
    author_url: str = "Unknown"
    plugin_url: str = "Unknown"
    min_cat_version: str = None
    max_cat_version: str = "Unknown"
    local_info: Dict = Field(default_factory=dict)
    dependencies: List[str] = Field(default_factory=list)