import time
from typing import List, Literal, Dict, Any
from langchain_core.callbacks import BaseCallbackHandler
from pydantic import BaseModel, Field, ConfigDict
from langchain_core.messages import BaseMessage
from langchain_core.outputs.llm_result import LLMResult
import tiktoken

from cheshirecat.log import log


class ModelInteraction(BaseModel):
    """
    Base class for interactions with models, capturing essential attributes common to all model interactions.

    Attributes
    ----------
    model_type : Literal["llm", "embedder"]
        The type of model involved in the interaction, either a large language model (LLM) or an embedder.
    source : str
        The source from which the interaction originates.
    prompt: List[str]
        The prompt or input provided to the model.
    input_tokens : int
        The number of input tokens processed by the model.
    started_at : float
        The timestamp when the interaction started. Defaults to the current time.
    """
    model_type: Literal["llm", "embedder"]
    source: str
    prompt: List[str]
    input_tokens: int
    started_at: float = Field(default_factory=lambda: time.time())

    model_config = ConfigDict(
        protected_namespaces=()
    )


class LLMModelInteraction(ModelInteraction):
    """
    Represents an interaction with a large language model (LLM).

    Inherits from ModelInteraction and adds specific attributes related to LLM interactions.

    Attributes
    ----------
    model_type : Literal["llm"]
        The type of model, which is fixed to "llm".
    reply : str
        The response generated by the LLM.
    output_tokens : int
        The number of output tokens generated by the LLM.
    ended_at : float
        The timestamp when the interaction ended.
    """
    model_type: Literal["llm"] = Field(default="llm")
    reply: str
    output_tokens: int
    ended_at: float


class EmbedderModelInteraction(ModelInteraction):
    """
    Represents an interaction with an embedding model.

    Inherits from ModelInteraction and includes attributes specific to embedding interactions.

    Attributes
    ----------
    model_type : Literal["embedder"]
        The type of model, which is fixed to "embedder".
    source : str
        The source of the interaction, defaulting to "recall".
    """
    model_type: Literal["embedder"] = Field(default="embedder")
    source: str = Field(default="recall")


class ModelInteractionHandler(BaseCallbackHandler):
    """
    Langchain callback handler for tracking model interactions.
    """
    def __init__(self, stray: "StrayCat", source: str):
        """
        Args:
            stray: StrayCat instance
            source: Source of the model interaction
        """
        self.stray = stray
        self.stray.working_memory.model_interactions.append(
            LLMModelInteraction(
                source=source,
                prompt=[],
                reply="",
                input_tokens=0,
                output_tokens=0,
                ended_at=0,
            )
        )

    def _count_tokens(self, text: str) -> int:
        # cl100k_base is the most common encoding for OpenAI models such as GPT-3.5, GPT-4 - what about other providers?
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))

    def on_chat_model_start(self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs) -> None:
        input_tokens = 0
        input_prompt = []
        # guide here: https://platform.openai.com/docs/guides/vision/calculating-costs#calculating-costs
        for m in messages[0]:
            if isinstance(m.content, str):
                input_tokens += self._count_tokens(m.content)
                input_prompt.append(m.content)
                continue

            if isinstance(m.content, list):
                for c in m.content:
                    if c["type"] == "text":
                        input_tokens += self._count_tokens(c["text"])
                        input_prompt.append(c["text"])
                        continue

                    if c["type"] == "image_url":
                        # TODO V2: how do we count image tokens?
                        log.warning("Could not count tokens for image message")
                        # do not send back to the client the whole base64 image
                        input_prompt.append("(image, tokens not counted)")
                        continue

                    log.warning(f"Could not count tokens for message type {c['type']}")

        self.last_interaction.input_tokens = int(input_tokens * 1.2) # You never know
        self.last_interaction.prompt = input_prompt

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        self.last_interaction.output_tokens = self._count_tokens(response.generations[0][0].text)
        self.last_interaction.reply = response.generations[0][0].text
        self.last_interaction.ended_at = time.time()

    @property
    def last_interaction(self) -> LLMModelInteraction:
        return self.stray.working_memory.model_interactions[-1]
